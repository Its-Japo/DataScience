{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 2. Redes Neuronales con MNIST\n",
        "- Jose Merida\n",
        "- Joaquin Puente"
      ],
      "metadata": {
        "id": "05eRPqRFlbLz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTiTP8eihus0"
      },
      "source": [
        "# Preparacion de Datos\n",
        "Por motivos de brevedad, vamos a colocar el procesamiento de datos dentro de una unica celda de Jupyter comentando que hace cada bloque de codigo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "y6eW47Y6hus0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc496f4-75f1-407b-987e-6f810b79da6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carga de datos\n",
        "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Confirmar forma de los conjuntos de datos\n",
        "assert X_entreno.shape == (60000, 28, 28)\n",
        "assert X_prueba.shape == (10000, 28, 28)\n",
        "assert y_entreno.shape == (60000,)\n",
        "assert y_prueba.shape == (10000,)\n",
        "\n",
        "# Crear conjunto de validacion\n",
        "num_obs_validacion = y_prueba.shape[0]\n",
        "num_obs_prueba = y_prueba.shape[0]\n",
        "\n",
        "# Normalizar datos\n",
        "X_entreno_normalizado = X_entreno / 255\n",
        "X_prueba_normalizado = X_prueba / 255\n",
        "\n",
        "# Separacion entrenamiento y validacion / batches\n",
        "X_validacion = X_entreno_normalizado[-num_obs_validacion: , : , : ]\n",
        "y_validacion = y_entreno[-num_obs_validacion:]\n",
        "X_entreno = X_entreno_normalizado[ : X_entreno_normalizado.shape[0] - num_obs_validacion, : , : ]\n",
        "y_entreno = y_entreno[ : y_entreno.shape[0] - num_obs_validacion]\n",
        "num_obs_entreno = y_entreno.shape[0]\n",
        "\n",
        "# Conversion a tensores\n",
        "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
        "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
        "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
        "\n",
        "# Barajeo de tandas conjunto de entrenamiento\n",
        "TAMANIO_TANDA = 100\n",
        "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
        "\n",
        "# Tandas de validacion y prueba (sin barajear)\n",
        "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
        "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
        "\n",
        "# Definir entrada / salida del modelo\n",
        "tamanio_entrada = 784\n",
        "tamanio_salida = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Baseline\n",
        "Empezando por el modelo delineado en el codigo visto en clase"
      ],
      "metadata": {
        "id": "HtqgFNvDlx-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tamanio_capa_escondida = 50\n",
        "\n",
        "# Arquitectura del modelo\n",
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'),\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "\n",
        "# Optimizaror, funcion de perdida y metricas\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "# Prueba del modelo\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "DxfJ_JBbl6HK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50fa0dd-1882-4b38-8640-35a73f8d955a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 7ms/step - accuracy: 0.8799 - loss: 0.4260 - val_accuracy: 0.9376 - val_loss: 0.2150\n",
            "Epoch 2/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9429 - loss: 0.1965 - val_accuracy: 0.9490 - val_loss: 0.1762\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9567 - loss: 0.1476 - val_accuracy: 0.9617 - val_loss: 0.1402\n",
            "Epoch 4/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9644 - loss: 0.1190 - val_accuracy: 0.9654 - val_loss: 0.1236\n",
            "Epoch 5/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9702 - loss: 0.0985 - val_accuracy: 0.9646 - val_loss: 0.1233\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 21.1766\n",
            "Pérdida de prueba: 20.47. Precisión de prueba: 96.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo baseline de por si ya tiene una precisión bastante alta, además no parece haber overfitting ya que los valores de precisión se mantienen consistentes a lo largo de los conjuntos de prueba, validación y entrenamiento."
      ],
      "metadata": {
        "id": "9qretedSny0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Modificación Ancho de la Red\n",
        "Para este inciso estaremos trabajando sobre el modelo Baseline proporcionado, teniendo dos capas \"baseline\" utilizando diferentes tamaños de capas escondidas. El objetivo es encontrar diferencias en rendimiento y tiempo de ejecución utilizando diferentes anchos de capa."
      ],
      "metadata": {
        "id": "8aBx07Zpi7Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 200 Neuronas"
      ],
      "metadata": {
        "id": "SLrN_6GMkXGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "50CHIf10h9fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946d14e9-9311-4eeb-967c-ea102a88defb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 7ms/step - accuracy: 0.9169 - loss: 0.2881 - val_accuracy: 0.9637 - val_loss: 0.1237\n",
            "Epoch 2/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9669 - loss: 0.1097 - val_accuracy: 0.9670 - val_loss: 0.1029\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9785 - loss: 0.0710 - val_accuracy: 0.9744 - val_loss: 0.0844\n",
            "Epoch 4/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9837 - loss: 0.0527 - val_accuracy: 0.9750 - val_loss: 0.0832\n",
            "Epoch 5/5\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9881 - loss: 0.0372 - val_accuracy: 0.9792 - val_loss: 0.0743\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 17.3350\n",
            "Pérdida de prueba: 14.93. Precisión de prueba: 97.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 200 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.66%\n",
        "- Precisión validación: 97.92%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Al aumentar el tamaño de las capas, el modelo logra realizar mejores predicciones. La precisión es significativamente más alta, sin embargo podemos empezar a ver una diferencia ligeramente más marcada entre el conjunto de entrenamiento y los conjuntos de prueba. Esto nos indica que con capas más anchas, el modelo es capaz de reconocer patrones más complejos presentes dentro del conjunto de datos."
      ],
      "metadata": {
        "id": "xcub9sJJpD-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50 Neuronas"
      ],
      "metadata": {
        "id": "oERNDCFJki6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "ex92WaBBjEix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350799ae-93c9-4a8d-8f8e-a19478462c89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.8818 - loss: 0.4251 - val_accuracy: 0.9413 - val_loss: 0.2047\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9434 - loss: 0.1916 - val_accuracy: 0.9543 - val_loss: 0.1586\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9566 - loss: 0.1465 - val_accuracy: 0.9620 - val_loss: 0.1337\n",
            "Epoch 4/5\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9641 - loss: 0.1189 - val_accuracy: 0.9662 - val_loss: 0.1191\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9706 - loss: 0.0978 - val_accuracy: 0.9628 - val_loss: 0.1248\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9577 - loss: 19.8733\n",
            "Pérdida de prueba: 16.94. Precisión de prueba: 96.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 50 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 96.62%\n",
        "- Precisión validación: 96.28%\n",
        "- Tiempo de ejecución: 10s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Este modelo es idéntico al modelo baseline, tiene un rendimiento bastante aceptable y el tiempo de ejecución de la celda es corto. Podemos ver un poco de variación con la pérdida en el conjunto de evaluación, sin embargo en este caso podemos tener casi por seguro que es debido a ruido y no hay overfitting o similar."
      ],
      "metadata": {
        "id": "3mWJv_WNpG8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100 Neuronas"
      ],
      "metadata": {
        "id": "Q4PsU5YXklAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "a5XCd_DpjYhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da39632-bc34-429e-a744-33962bdce695"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9008 - loss: 0.3478 - val_accuracy: 0.9560 - val_loss: 0.1566\n",
            "Epoch 2/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9582 - loss: 0.1390 - val_accuracy: 0.9654 - val_loss: 0.1128\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9703 - loss: 0.0987 - val_accuracy: 0.9706 - val_loss: 0.1019\n",
            "Epoch 4/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9776 - loss: 0.0728 - val_accuracy: 0.9724 - val_loss: 0.0964\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9818 - loss: 0.0579 - val_accuracy: 0.9720 - val_loss: 0.0917\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 15.7796\n",
            "Pérdida de prueba: 14.62. Precisión de prueba: 97.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 100 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.29%\n",
        "- Precisión validación: 97.24%\n",
        "- Tiempo de ejecución: 9s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Utilizando 100 neuronas por capa, podemos ver que los resultados son bastante similares a los de 200 neuronas. El tiempo de ejecución es ligeramente menor (9s vs 10s) y la precisión disminuye ligeramente."
      ],
      "metadata": {
        "id": "Ew6Fvf-XpLXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 300 Neuronas"
      ],
      "metadata": {
        "id": "K7wXXeG7kmke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(300, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(300, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "YFqBGulHjbeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c74903-fac4-4fd6-c78a-cd3d5d874cb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9264 - loss: 0.2519 - val_accuracy: 0.9608 - val_loss: 0.1302\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9713 - loss: 0.0941 - val_accuracy: 0.9728 - val_loss: 0.0920\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9810 - loss: 0.0607 - val_accuracy: 0.9759 - val_loss: 0.0786\n",
            "Epoch 4/5\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9860 - loss: 0.0437 - val_accuracy: 0.9770 - val_loss: 0.0764\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9897 - loss: 0.0322 - val_accuracy: 0.9780 - val_loss: 0.0776\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 15.1816\n",
            "Pérdida de prueba: 12.93. Precisión de prueba: 97.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 300 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.79%\n",
        "- Precisión validación: 97.80%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Hasta el momento, un mayor ancho sigue dando mejores resultados. Los tiempos de entrenamiento son similares, pero a una mayor escala puede que sea algo problemático. En este caso, el descenso de gradiente parece converger bastante bien a pesar de un poco de ruido en la pérdida entre la época 4 y 5. El tiempo de ejecución es mayor a los anteriores, y aquí empezamos a ver una ligera discrepancia entre la precisión en el set de entrenamiento vs de validación. Sin embargo, hasta el momento sigue sin ser algo que realmente nos preocupe."
      ],
      "metadata": {
        "id": "VSRKSm_JpVWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 500 Neuronas"
      ],
      "metadata": {
        "id": "TBrKGFDDkoXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(500, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(500, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "gdff_pqqjdeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979857c2-ebef-4104-c779-30333e2bf502"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9324 - loss: 0.2277 - val_accuracy: 0.9663 - val_loss: 0.1175\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9746 - loss: 0.0830 - val_accuracy: 0.9758 - val_loss: 0.0811\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9827 - loss: 0.0542 - val_accuracy: 0.9783 - val_loss: 0.0800\n",
            "Epoch 4/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9888 - loss: 0.0361 - val_accuracy: 0.9776 - val_loss: 0.0828\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9923 - loss: 0.0240 - val_accuracy: 0.9799 - val_loss: 0.0805\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 16.0120\n",
            "Pérdida de prueba: 13.42. Precisión de prueba: 97.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.99%\n",
        "- Precisión validación: 96.39%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Utilizando 500 neuronas parece existir un ligero sobreajuste, en este caso podemos ver cómo la precisión de entrenamiento sigue subiendo mientras que la de validación se queda estancada. Sin embargo, hasta el momento seguimos viendo ganancias al tener capas anchas."
      ],
      "metadata": {
        "id": "uOLw6QHqpXSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones Ancho de Red\n",
        "En este caso, encontramos una relación bastante clara entre ancho de red y rendimiento. Desde 50-300 neuronas, encontramos que los modelos con un mayor ancho proporcionan un mejor rendimiento a un ligero costo de tiempo de ejecución. Sin embargo, para un ancho de 500 neuronas nos dimos cuenta que existe sobreajuste y el modelo puede llegar a adaptarse demasiado a los datos de entrenamiento. En las épocas posteriores, empieza a aparecer una discrepancia entre las precisiones de entrenamiento y validación. Para la optimización final de modelos, podemos tomar lo que aprendimos y experimentar con capas de diferentes tamaños o quedarnos dentro del rango de 250-500 generalmente para evitar sobreajustes. Se puede experimentar con una capa inicial ancha, seguida de capas más angostas."
      ],
      "metadata": {
        "id": "o5jTfDXtANvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Modificación de la Profundidad de Red\n",
        "Utilizando lo observado en el inciso anterior, aquí buscamos identificar cómo la profundidad de la red puede afectar el rendimiento y tiempos de ejecución. Además, podemos intentar identificar una relación en cómo la profundidad afecta la relación entre ancho y rendimiento. Por ejemplo, un ancho y una profundidad altos a la vez pueden llevar a un sobreajuste."
      ],
      "metadata": {
        "id": "0BZ2UGWklbkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "QT5FDGNdjG7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918b0ac6-c7e2-413e-e2db-27196dd8b356"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 3s - 7ms/step - accuracy: 0.8794 - loss: 0.4013 - val_accuracy: 0.9407 - val_loss: 0.1982\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9459 - loss: 0.1793 - val_accuracy: 0.9585 - val_loss: 0.1438\n",
            "Epoch 3/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9590 - loss: 0.1356 - val_accuracy: 0.9642 - val_loss: 0.1258\n",
            "Epoch 4/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9675 - loss: 0.1082 - val_accuracy: 0.9667 - val_loss: 0.1122\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9724 - loss: 0.0915 - val_accuracy: 0.9652 - val_loss: 0.1173\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9597 - loss: 22.1490\n",
            "Pérdida de prueba: 20.21. Precisión de prueba: 96.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 3 capas de 50 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 96.50%\n",
        "- Precisión validación: 96.67%\n",
        "- Tiempo de ejecución: 10s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El modelo ofrece un rendimiento bastante similar en comparación a su contraparte con anchos idénticos y una capa menos. Esto puede deberse a que una capa adicional realmente no es de mucha ayuda cuando las capas no incluyen la complejidad necesaria. En el siguiente inciso podemos examinar cómo afecta aumentar aún más la profundidad utilizando anchos de capa pequeños."
      ],
      "metadata": {
        "id": "I17VG_Yfpg6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    tf.keras.layers.Dense(250, activation='relu'),\n",
        "    tf.keras.layers.Dense(250, activation='relu'),\n",
        "    tf.keras.layers.Dense(250, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "Nx_wEXOFr1pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4416c7f5-9697-404e-80c7-092b4d89f43e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9209 - loss: 0.2685 - val_accuracy: 0.9622 - val_loss: 0.1246\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9686 - loss: 0.1017 - val_accuracy: 0.9685 - val_loss: 0.1003\n",
            "Epoch 3/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9790 - loss: 0.0667 - val_accuracy: 0.9773 - val_loss: 0.0802\n",
            "Epoch 4/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9850 - loss: 0.0482 - val_accuracy: 0.9753 - val_loss: 0.0909\n",
            "Epoch 5/5\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9886 - loss: 0.0365 - val_accuracy: 0.9765 - val_loss: 0.0851\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9691 - loss: 22.1030\n",
            "Pérdida de prueba: 18.01. Precisión de prueba: 97.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 3 capas de 250 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.35%\n",
        "- Precisión validación: 97.73%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Los resultados son bastante similares a la versión con 2 capas, puede que este set de datos no se vea beneficiado de modelos más profundos. En este caso también logramos ver señales de overfitting, dónde empieza a aparecer una diferencia entre los valores de precisión de entrenamiento y validación. Mientras los de entrenamiento siguen aimentando ligeramente, los de validación se quedan iguales o peores."
      ],
      "metadata": {
        "id": "-Us2iJCFWI44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Redes Profundas\n",
        "Utilizando lo que hemos observado en incisos anteriores, podemos probar una red profunda con un ancho bajo para intentar identificar mejoras, seguido de una un poco más ancha e identificar cuál tiene un mejor rendimiento. Luego, intentar agregar una capa adicional a la que mejor rendimiento tenga y comparar su rendimiento."
      ],
      "metadata": {
        "id": "dHyGNuTelg_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(50, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "xndRWtpUP0R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d799727a-5002-4ef3-fce6-b9bf7204667c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 7ms/step - accuracy: 0.8712 - loss: 0.4249 - val_accuracy: 0.9363 - val_loss: 0.2197\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9483 - loss: 0.1735 - val_accuracy: 0.9623 - val_loss: 0.1330\n",
            "Epoch 3/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9619 - loss: 0.1276 - val_accuracy: 0.9638 - val_loss: 0.1318\n",
            "Epoch 4/5\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9682 - loss: 0.1042 - val_accuracy: 0.9640 - val_loss: 0.1252\n",
            "Epoch 5/5\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.9743 - loss: 0.0865 - val_accuracy: 0.9698 - val_loss: 0.1084\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 23.5582\n",
            "Pérdida de prueba: 19.43. Precisión de prueba: 96.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 96.40%\n",
        "- Precisión validación: 96.98%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El rendimiento sigue siendo similar a sus contrapartes con profundidades diferentes, de momento al tener capas pequeñas no encontramos ningún beneficio al tener modelos más profundos."
      ],
      "metadata": {
        "id": "OxxDizHaWJyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "KJ5QJHgEljes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d6ecfa-0b28-40d4-915b-1c1a4b042113"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9200 - loss: 0.2663 - val_accuracy: 0.9630 - val_loss: 0.1293\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9682 - loss: 0.1017 - val_accuracy: 0.9666 - val_loss: 0.1093\n",
            "Epoch 3/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9777 - loss: 0.0717 - val_accuracy: 0.9692 - val_loss: 0.1042\n",
            "Epoch 4/5\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9835 - loss: 0.0528 - val_accuracy: 0.9730 - val_loss: 0.0960\n",
            "Epoch 5/5\n",
            "500/500 - 2s - 3ms/step - accuracy: 0.9856 - loss: 0.0437 - val_accuracy: 0.9686 - val_loss: 0.1199\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 19.4594\n",
            "Pérdida de prueba: 17.25. Precisión de prueba: 97.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 4 capas de 250 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.43%\n",
        "- Precisión validación: 96.39%\n",
        "- Tiempo de ejecución: 11s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El rendimiento fue bastante bueno, sin embargo prueba ser inferior a arquitecturas distintas. Esta arquitectura presenta un caso de overfitting bastante severo, dónde la precisión de entrenamiento aumenta de 92% hasta 98.56% mientras la de validación continúa en los mismos valores o hasta disminuye. Podemos predecir que al utilizar un modelo aún más profundo el overfitting será aún más claro."
      ],
      "metadata": {
        "id": "-8LmMq0QWK_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "    tf.keras.layers.Dense(250, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "tB6LWB03sqpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd519f1-cb7b-4385-f3c1-e879324c9ecd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 - 4s - 8ms/step - accuracy: 0.9178 - loss: 0.2689 - val_accuracy: 0.9579 - val_loss: 0.1336\n",
            "Epoch 2/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9663 - loss: 0.1098 - val_accuracy: 0.9652 - val_loss: 0.1088\n",
            "Epoch 3/5\n",
            "500/500 - 2s - 4ms/step - accuracy: 0.9757 - loss: 0.0779 - val_accuracy: 0.9731 - val_loss: 0.0894\n",
            "Epoch 4/5\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9818 - loss: 0.0597 - val_accuracy: 0.9701 - val_loss: 0.1132\n",
            "Epoch 5/5\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9844 - loss: 0.0500 - val_accuracy: 0.9762 - val_loss: 0.0839\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 19.6445\n",
            "Pérdida de prueba: 16.92. Precisión de prueba: 97.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 5 capas de 250 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.37%\n",
        "- Precisión validación: 97.62%\n",
        "- Tiempo de ejecución: 14s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "En este caso nos damos cuenta que nuestra predicción fue correcta, en este modelo nuevamente encontramos overfitting y dejamos de encontrar mejoras en cuanto a precisión dentro del conjunto de validación. Además, el tiempo de ejecución fue mayor en comparación a los demás."
      ],
      "metadata": {
        "id": "tI6ph7xtWMJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones Profundidad\n",
        "En cuanto a la profundidad, realmente no existieron diferencias positivas al implementar capas adicionales a los modelos. Utilizando anchos más pequeños, la precisión de los modelos fue bastante similar. Mientras que utilizando anchos más altos, los modelos tenían una tendencia más alta a sobreajustarse al conjunto de prueba. Podemos intuir que lo mejor sería seguir utilizando 2-3 capas y posiblemente jugar con los tamaños de las capas posteriores para reducir el overfitting que puede llegar a ocurrir. Esto ya que los modelos hasta el momento se han visto beneficiados de un ancho más alto, pero al aumentar más la profundidad se sobreajustan."
      ],
      "metadata": {
        "id": "ZXUkSCpy7bn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Funciones de Activación I\n",
        "Para este inciso, estamos utilizando el modelo baseline y 10 épocas en lugar de 5. Esto con la finalidad de poder comparar las velocidades de convergencia, dándole \"tiempo\" a todos los modelos que puedan converger."
      ],
      "metadata": {
        "id": "q394l_-QpRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 10\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "NVkBPDWLpYTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a984bf14-9b2f-4e3c-8ecb-a97ba96211bf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 - 39s - 77ms/step - accuracy: 0.7679 - loss: 1.0742 - val_accuracy: 0.9033 - val_loss: 0.4260\n",
            "Epoch 2/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9076 - loss: 0.3557 - val_accuracy: 0.9283 - val_loss: 0.2683\n",
            "Epoch 3/10\n",
            "500/500 - 5s - 11ms/step - accuracy: 0.9275 - loss: 0.2577 - val_accuracy: 0.9401 - val_loss: 0.2147\n",
            "Epoch 4/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9395 - loss: 0.2110 - val_accuracy: 0.9495 - val_loss: 0.1847\n",
            "Epoch 5/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9479 - loss: 0.1804 - val_accuracy: 0.9539 - val_loss: 0.1680\n",
            "Epoch 6/10\n",
            "500/500 - 5s - 11ms/step - accuracy: 0.9546 - loss: 0.1576 - val_accuracy: 0.9566 - val_loss: 0.1536\n",
            "Epoch 7/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9591 - loss: 0.1401 - val_accuracy: 0.9605 - val_loss: 0.1429\n",
            "Epoch 8/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9637 - loss: 0.1256 - val_accuracy: 0.9609 - val_loss: 0.1339\n",
            "Epoch 9/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9674 - loss: 0.1133 - val_accuracy: 0.9621 - val_loss: 0.1293\n",
            "Epoch 10/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9710 - loss: 0.1027 - val_accuracy: 0.9646 - val_loss: 0.1244\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9475 - loss: 0.1658\n",
            "Pérdida de prueba: 0.15. Precisión de prueba: 95.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 50 neuronas con Sigmoid\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 95.45%\n",
        "- Precisión validación: 96.46%\n",
        "- Tiempo de ejecución: 60s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El rendimiento entre sigmoid y ReLU para capas idénticas es bastante similar (pero ReLU es superior), sin embargo encontramos que la velocidad de convergencia es significativamente más lenta utilizando sigmoid. Adicionalmente, el tiempo de entrenamiento aumenta en comparación a ReLU. Si se tiene rendimiento y tiempo de ejecución en mente, sugerimos optar por ReLU."
      ],
      "metadata": {
        "id": "nWuTZ5pKWNPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Funciones de Activación II\n",
        "Para este inciso seguiremos trabajando con el modelo baseline y 10 épocas"
      ],
      "metadata": {
        "id": "Op09UDPGpgzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='tanh'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 10\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "eC-Xq5oapjKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfe397a-e1da-45da-9911-4e1a5cf3d435"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 - 5s - 9ms/step - accuracy: 0.8819 - loss: 0.4352 - val_accuracy: 0.9451 - val_loss: 0.1986\n",
            "Epoch 2/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9481 - loss: 0.1798 - val_accuracy: 0.9566 - val_loss: 0.1559\n",
            "Epoch 3/10\n",
            "500/500 - 4s - 9ms/step - accuracy: 0.9599 - loss: 0.1313 - val_accuracy: 0.9649 - val_loss: 0.1244\n",
            "Epoch 4/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9690 - loss: 0.1036 - val_accuracy: 0.9671 - val_loss: 0.1141\n",
            "Epoch 5/10\n",
            "500/500 - 6s - 11ms/step - accuracy: 0.9745 - loss: 0.0857 - val_accuracy: 0.9711 - val_loss: 0.1005\n",
            "Epoch 6/10\n",
            "500/500 - 5s - 9ms/step - accuracy: 0.9785 - loss: 0.0719 - val_accuracy: 0.9708 - val_loss: 0.1051\n",
            "Epoch 7/10\n",
            "500/500 - 6s - 12ms/step - accuracy: 0.9820 - loss: 0.0616 - val_accuracy: 0.9714 - val_loss: 0.0991\n",
            "Epoch 8/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9845 - loss: 0.0528 - val_accuracy: 0.9716 - val_loss: 0.0965\n",
            "Epoch 9/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9860 - loss: 0.0464 - val_accuracy: 0.9731 - val_loss: 0.0964\n",
            "Epoch 10/10\n",
            "500/500 - 3s - 7ms/step - accuracy: 0.9881 - loss: 0.0397 - val_accuracy: 0.9718 - val_loss: 0.0977\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9588 - loss: 0.1525\n",
            "Pérdida de prueba: 0.14. Precisión de prueba: 96.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 96.34%\n",
        "- Precisión validación: 97.31%\n",
        "- Tiempo de ejecución: 46s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El rendimiento realmente es parecido a utilizar 2 capas ReLU, en este caso la diferencia es bastante baja y el tiempo de ejecución es alrededor de 4-5 veces más tardado en comparación. Las capas utilizando tanh tardan más en converger y tienden a variar menos, por lo que pueden llegar a ser útiles como \"capa final\" en algunos modelos. Sin embargo, en este caso al únicamente tener 2 capas no vemos beneficio alguno de un approach más conservador. Al utilizar ReLU, podemos captar los features de manera un poco más agresiva. De cierta manera, con las configuraciones utilizadas las ventajas y desventajas de cada uno se \"cancelan\" y resultan en un rendimiento similar."
      ],
      "metadata": {
        "id": "xKTl0eJDWOZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Tamaño de Batch Grande"
      ],
      "metadata": {
        "id": "ceUtb3w4pnhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAMANIO_TANDA = 10000\n",
        "\n",
        "datos_entreno = datos_entreno.unbatch().shuffle(buffer_size=num_obs_entreno).batch(TAMANIO_TANDA)\n",
        "datos_validacion = datos_validacion.unbatch().batch(TAMANIO_TANDA)\n",
        "datos_prueba = datos_prueba.unbatch().batch(TAMANIO_TANDA)\n"
      ],
      "metadata": {
        "id": "TO-o9czWbpta"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "3LXtBK1Lbs6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560a6303-a615-4786-93cb-a76e54e462a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 - 3s - 687ms/step - accuracy: 0.2447 - loss: 2.2125 - val_accuracy: 0.3867 - val_loss: 2.0438\n",
            "Epoch 2/5\n",
            "5/5 - 1s - 163ms/step - accuracy: 0.4508 - loss: 1.9371 - val_accuracy: 0.5611 - val_loss: 1.7490\n",
            "Epoch 3/5\n",
            "5/5 - 3s - 508ms/step - accuracy: 0.5838 - loss: 1.6270 - val_accuracy: 0.6473 - val_loss: 1.4169\n",
            "Epoch 4/5\n",
            "5/5 - 1s - 251ms/step - accuracy: 0.6566 - loss: 1.3111 - val_accuracy: 0.7070 - val_loss: 1.1174\n",
            "Epoch 5/5\n",
            "5/5 - 1s - 254ms/step - accuracy: 0.7219 - loss: 1.0451 - val_accuracy: 0.7854 - val_loss: 0.8790\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - accuracy: 0.7818 - loss: 39.6636\n",
            "Pérdida de prueba: 39.66. Precisión de prueba: 78.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 10,000\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 78.18%\n",
        "- Precisión validación: 78.54%\n",
        "- Tiempo de ejecución: 9s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Utilizar un batch size excesivamente grande, cómo lo es este ejemplo, puede resultar en un modelo demasiado \"tonto\". Dónde el modelo puede ser entrenado rápidamente pero generaliza demasiado y no es capaz de converger a los mínimos que debería."
      ],
      "metadata": {
        "id": "4XE9h05Tbikt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Descenso de Gradiente Estocástico"
      ],
      "metadata": {
        "id": "rPV5J7Eqpvt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAMANIO_TANDA = 1\n",
        "tamanio_capa_escondida = 50\n",
        "datos_entreno = datos_entreno.unbatch().shuffle(buffer_size=num_obs_entreno).batch(TAMANIO_TANDA)\n",
        "datos_validacion = datos_validacion.unbatch().batch(TAMANIO_TANDA)\n",
        "datos_prueba = datos_prueba.unbatch().batch(TAMANIO_TANDA)"
      ],
      "metadata": {
        "id": "KRWVPV-Eby68"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 5\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "XM3beWs4b1FD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e70aa82f-b50d-43b4-c09f-7ee190ef0e3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 - 137s - 3ms/step - accuracy: 0.9225 - loss: 0.2596 - val_accuracy: 0.9457 - val_loss: 0.1840\n",
            "Epoch 2/5\n",
            "50000/50000 - 126s - 3ms/step - accuracy: 0.9564 - loss: 0.1594 - val_accuracy: 0.9625 - val_loss: 0.1404\n",
            "Epoch 3/5\n",
            "50000/50000 - 142s - 3ms/step - accuracy: 0.9614 - loss: 0.1426 - val_accuracy: 0.9672 - val_loss: 0.1289\n",
            "Epoch 4/5\n",
            "50000/50000 - 140s - 3ms/step - accuracy: 0.9657 - loss: 0.1304 - val_accuracy: 0.9672 - val_loss: 0.1369\n",
            "Epoch 5/5\n",
            "50000/50000 - 144s - 3ms/step - accuracy: 0.9681 - loss: 0.1238 - val_accuracy: 0.9644 - val_loss: 0.1671\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'perdida_prueba' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1247076696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m           verbose = 2)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdida_prueba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_prueba\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'perdida_prueba' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 1\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 78.18%\n",
        "- Precisión validación: 78.54%\n",
        "- Tiempo de ejecución: 9s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Al utilizar un batch size de 1, el tiempo de ejecución es sumamente alto y es sumamente lento para converger. En este caso, nos vimos limitados con el número de épocas para entrenar el modelo debido a los tiempos excesivos de ejecución. En teoría, este tipo de modelos son sumamente efectivos al poder evitar mínimos locales. Sin embargo, en práctica, su implementación resulta en tiempos de ejecución excesivamente altos y una convergencia demasiado lenta."
      ],
      "metadata": {
        "id": "15FItczwAXtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Tasa de Aprendizaje Baja"
      ],
      "metadata": {
        "id": "Goj4PfhMpzQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAMANIO_TANDA = 100\n",
        "\n",
        "datos_entreno = datos_entreno.unbatch().shuffle(buffer_size=num_obs_entreno).batch(TAMANIO_TANDA)\n",
        "datos_validacion = datos_validacion.unbatch().batch(TAMANIO_TANDA)\n",
        "datos_prueba = datos_prueba.unbatch().batch(TAMANIO_TANDA)\n",
        "\n",
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "optimizador = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "modelo.compile(optimizer=optimizador, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 10\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "kIcztPVDb5IC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73fab61-ab4f-4cab-cc62-a1709cc54585"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 - 4s - 9ms/step - accuracy: 0.6307 - loss: 1.3269 - val_accuracy: 0.8621 - val_loss: 0.5945\n",
            "Epoch 2/10\n",
            "500/500 - 3s - 7ms/step - accuracy: 0.8709 - loss: 0.4936 - val_accuracy: 0.9028 - val_loss: 0.3673\n",
            "Epoch 3/10\n",
            "500/500 - 4s - 9ms/step - accuracy: 0.9007 - loss: 0.3601 - val_accuracy: 0.9177 - val_loss: 0.3017\n",
            "Epoch 4/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9144 - loss: 0.3068 - val_accuracy: 0.9259 - val_loss: 0.2675\n",
            "Epoch 5/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9221 - loss: 0.2752 - val_accuracy: 0.9309 - val_loss: 0.2477\n",
            "Epoch 6/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9285 - loss: 0.2527 - val_accuracy: 0.9361 - val_loss: 0.2295\n",
            "Epoch 7/10\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9328 - loss: 0.2352 - val_accuracy: 0.9402 - val_loss: 0.2173\n",
            "Epoch 8/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9370 - loss: 0.2207 - val_accuracy: 0.9423 - val_loss: 0.2060\n",
            "Epoch 9/10\n",
            "500/500 - 8s - 15ms/step - accuracy: 0.9401 - loss: 0.2081 - val_accuracy: 0.9470 - val_loss: 0.1953\n",
            "Epoch 10/10\n",
            "500/500 - 5s - 11ms/step - accuracy: 0.9439 - loss: 0.1974 - val_accuracy: 0.9476 - val_loss: 0.1890\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 29.5485\n",
            "Pérdida de prueba: 27.00. Precisión de prueba: 94.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 50 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 94.06%\n",
        "- Precisión validación: 94.76%\n",
        "- Tiempo de ejecución: 14s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "En este caso, la velocidad de convergencia del modelo fue bastante lenta. A pesar de haber agregado épocas adicionales, parece no terminar de converger. Adicionalmente, este tipo de modelos con tasas de aprendizajes bajas suele quedarse \"atascado\" en mínimos locales."
      ],
      "metadata": {
        "id": "T0o20lwYWSY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Tasa de Aprendizaje Alta"
      ],
      "metadata": {
        "id": "n2tzMBF1p2W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
        "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
        "\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
        "])\n",
        "\n",
        "optimizador = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
        "\n",
        "modelo.compile(optimizer=optimizador, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "NUMERO_EPOCAS = 10\n",
        "\n",
        "modelo.fit(datos_entreno,\n",
        "          epochs = NUMERO_EPOCAS,\n",
        "          validation_data = datos_validacion,\n",
        "          verbose = 2)\n",
        "\n",
        "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
        "\n",
        "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
      ],
      "metadata": {
        "id": "6vTPpjeCcb7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462cbeeb-9b8b-446a-8cb2-a18c78d8c957"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.8998 - loss: 0.3318 - val_accuracy: 0.9392 - val_loss: 0.2128\n",
            "Epoch 2/10\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9420 - loss: 0.2017 - val_accuracy: 0.9448 - val_loss: 0.2072\n",
            "Epoch 3/10\n",
            "500/500 - 4s - 7ms/step - accuracy: 0.9480 - loss: 0.1845 - val_accuracy: 0.9483 - val_loss: 0.1977\n",
            "Epoch 4/10\n",
            "500/500 - 4s - 9ms/step - accuracy: 0.9536 - loss: 0.1710 - val_accuracy: 0.9527 - val_loss: 0.1883\n",
            "Epoch 5/10\n",
            "500/500 - 5s - 11ms/step - accuracy: 0.9543 - loss: 0.1666 - val_accuracy: 0.9506 - val_loss: 0.1977\n",
            "Epoch 6/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9566 - loss: 0.1597 - val_accuracy: 0.9519 - val_loss: 0.1954\n",
            "Epoch 7/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9578 - loss: 0.1589 - val_accuracy: 0.9562 - val_loss: 0.2020\n",
            "Epoch 8/10\n",
            "500/500 - 2s - 5ms/step - accuracy: 0.9601 - loss: 0.1519 - val_accuracy: 0.9494 - val_loss: 0.2311\n",
            "Epoch 9/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9615 - loss: 0.1486 - val_accuracy: 0.9518 - val_loss: 0.2136\n",
            "Epoch 10/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9636 - loss: 0.1431 - val_accuracy: 0.9592 - val_loss: 0.1767\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9446 - loss: 48.2395\n",
            "Pérdida de prueba: 39.88. Precisión de prueba: 95.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 2 capas de 500 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión entrenamiento: 95.25%\n",
        "- Precisión validación: 95.92%\n",
        "- Tiempo de ejecución: 13s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Utilizando learning rates más altos, se evita completamente quedar atascados en mínimos locales. El modelo puede moverse más \"bruscamente\" buscando soluciones más óptimas, sin embargo parece nunca poder \"decidirse\" para converger en el lugar correcto. En este caso vemos como la pérdida sigue variando y no encuentra un único lugar dónde enfocarse realmente."
      ],
      "metadata": {
        "id": "rJZkO0gQWVox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Optimización Avanzada\n",
        "Para aplicar L2 y Dropout, podemos empezar por un modelo bastante \"seguro\". Por \"seguro\" nos referimos a un modelo que no ha exhibido patrones de overfitting, pero tiene la complejidad suficiente donde una \"ayuda a generalizar\" puede probar su utilidad."
      ],
      "metadata": {
        "id": "Df846cUKp5XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularizacion Moderada"
      ],
      "metadata": {
        "id": "1sE9i6B6lfuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Modelo con regularización moderada\n",
        "# 1. Light Regularization Model (L2 + Dropout)\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "modelo.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\")\n",
        "modelo.fit(datos_entreno,\n",
        "                   epochs=10,\n",
        "                   validation_data=datos_validacion,\n",
        "                   verbose=2)\n",
        "\n",
        "perdida, precision = modelo.evaluate(datos_prueba)\n",
        "print(f'Precisión prueba: {precision*100:.2f}% | Pérdida: {perdida:.4f}\\n')"
      ],
      "metadata": {
        "id": "YAX-ZEhH9csa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d85cda-fee9-45be-f689-1fcc46258a7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\n",
            "Epoch 1/10\n",
            "500/500 - 8s - 15ms/step - accuracy: 0.9080 - loss: 0.6001 - val_accuracy: 0.9630 - val_loss: 0.3669\n",
            "Epoch 2/10\n",
            "500/500 - 7s - 15ms/step - accuracy: 0.9599 - loss: 0.3376 - val_accuracy: 0.9696 - val_loss: 0.2816\n",
            "Epoch 3/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9687 - loss: 0.2608 - val_accuracy: 0.9663 - val_loss: 0.2560\n",
            "Epoch 4/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9710 - loss: 0.2242 - val_accuracy: 0.9752 - val_loss: 0.2052\n",
            "Epoch 5/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9746 - loss: 0.1987 - val_accuracy: 0.9750 - val_loss: 0.1955\n",
            "Epoch 6/10\n",
            "500/500 - 5s - 11ms/step - accuracy: 0.9750 - loss: 0.1881 - val_accuracy: 0.9749 - val_loss: 0.1910\n",
            "Epoch 7/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9771 - loss: 0.1779 - val_accuracy: 0.9721 - val_loss: 0.1936\n",
            "Epoch 8/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9783 - loss: 0.1717 - val_accuracy: 0.9769 - val_loss: 0.1731\n",
            "Epoch 9/10\n",
            "500/500 - 8s - 16ms/step - accuracy: 0.9774 - loss: 0.1676 - val_accuracy: 0.9745 - val_loss: 0.1802\n",
            "Epoch 10/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9781 - loss: 0.1648 - val_accuracy: 0.9772 - val_loss: 0.1730\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 16.9600\n",
            "Precisión prueba: 96.34% | Pérdida: 15.2706\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 3 capas de 256 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.72%\n",
        "- Precisión validación: 96.34%\n",
        "- Tiempo de ejecución: 18s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "Comparando este modelo con un modelo similar sin Dropout ni L2, el rendimiento fue ligeramente peor. Sin embargo, logramos ver que parece existir menos ajuste en cuánto a los datos de entrenamiento. Por ejemplo, en el otro modelo encontramos precisión cercana al 99% en el set de entrenamiento mientras aquí quedó en 98% con resultados similares. Esto nos indica que este modelo si logra prevenir de cierta manera el overfitting, por lo cual se pueden tomar en cuenta estas técnicas para arquitecturas más complejas."
      ],
      "metadata": {
        "id": "wTCHHPryDJwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularización Alta con Dropout"
      ],
      "metadata": {
        "id": "gje00koZljbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Modelo con regularización moderada\n",
        "# 1. Light Regularization Model (L2 + Dropout)\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "modelo.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\")\n",
        "modelo.fit(datos_entreno,\n",
        "                   epochs=10,\n",
        "                   validation_data=datos_validacion,\n",
        "                   verbose=2)\n",
        "\n",
        "perdida, precision = modelo.evaluate(datos_prueba)\n",
        "print(f'Precisión prueba: {precision*100:.2f}% | Pérdida: {perdida:.4f}\\n')"
      ],
      "metadata": {
        "id": "rbb8Hn25l0os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba266f8-66b1-4611-c626-24d0c4f5008e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\n",
            "Epoch 1/10\n",
            "500/500 - 6s - 13ms/step - accuracy: 0.8701 - loss: 1.6939 - val_accuracy: 0.9206 - val_loss: 0.6515\n",
            "Epoch 2/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9138 - loss: 0.6468 - val_accuracy: 0.9394 - val_loss: 0.5475\n",
            "Epoch 3/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9244 - loss: 0.5800 - val_accuracy: 0.9448 - val_loss: 0.5050\n",
            "Epoch 4/10\n",
            "500/500 - 10s - 20ms/step - accuracy: 0.9291 - loss: 0.5408 - val_accuracy: 0.9460 - val_loss: 0.4791\n",
            "Epoch 5/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9320 - loss: 0.5143 - val_accuracy: 0.9456 - val_loss: 0.4675\n",
            "Epoch 6/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9340 - loss: 0.4956 - val_accuracy: 0.9444 - val_loss: 0.4619\n",
            "Epoch 7/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9380 - loss: 0.4742 - val_accuracy: 0.9582 - val_loss: 0.4091\n",
            "Epoch 8/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9385 - loss: 0.4633 - val_accuracy: 0.9502 - val_loss: 0.4288\n",
            "Epoch 9/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9400 - loss: 0.4561 - val_accuracy: 0.9457 - val_loss: 0.4227\n",
            "Epoch 10/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9410 - loss: 0.4431 - val_accuracy: 0.9582 - val_loss: 0.3986\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9161 - loss: 9.0092\n",
            "Precisión prueba: 92.46% | Pérdida: 7.8441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 3 capas de 256 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 92.46%\n",
        "- Precisión validación: 95.82%\n",
        "- Tiempo de ejecución: 18s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "El modelo parece tomar un approach demasiado \"generalizador\" con los parámetros utilizados, logra converger a una precisión específica bastante bien y el descenso de gradiente se ve bien también. Sin embargo, parece no ser capaz de llegar a una precisión alta. Esto nos indica que deberíamos probar parámetros un poco más bajos al utilizar L2 y dropouts, y además podríamos probar utilizar parámetros altos en las primeras capas y disminuirlos en las siguientes."
      ],
      "metadata": {
        "id": "g46vSaAAD1eI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Únicamente Dropout"
      ],
      "metadata": {
        "id": "--P8OBV-lrPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Modelo con regularización moderada\n",
        "# 1. Light Regularization Model (L2 + Dropout)\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "modelo.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\")\n",
        "modelo.fit(datos_entreno,\n",
        "                   epochs=10,\n",
        "                   validation_data=datos_validacion,\n",
        "                   verbose=2)\n",
        "\n",
        "perdida, precision = modelo.evaluate(datos_prueba)\n",
        "print(f'Precisión prueba: {precision*100:.2f}% | Pérdida: {perdida:.4f}\\n')"
      ],
      "metadata": {
        "id": "YHrllBFcl2xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec99fdf3-bad7-4811-d01d-5ec345709d4a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\n",
            "Epoch 1/10\n",
            "500/500 - 6s - 12ms/step - accuracy: 0.8975 - loss: 0.3370 - val_accuracy: 0.9615 - val_loss: 0.1248\n",
            "Epoch 2/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9579 - loss: 0.1412 - val_accuracy: 0.9704 - val_loss: 0.1013\n",
            "Epoch 3/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9681 - loss: 0.1037 - val_accuracy: 0.9759 - val_loss: 0.0811\n",
            "Epoch 4/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9735 - loss: 0.0850 - val_accuracy: 0.9769 - val_loss: 0.0802\n",
            "Epoch 5/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9775 - loss: 0.0711 - val_accuracy: 0.9793 - val_loss: 0.0716\n",
            "Epoch 6/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9791 - loss: 0.0646 - val_accuracy: 0.9786 - val_loss: 0.0748\n",
            "Epoch 7/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9828 - loss: 0.0553 - val_accuracy: 0.9771 - val_loss: 0.0806\n",
            "Epoch 8/10\n",
            "500/500 - 3s - 7ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9778 - val_loss: 0.0803\n",
            "Epoch 9/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9851 - loss: 0.0469 - val_accuracy: 0.9785 - val_loss: 0.0752\n",
            "Epoch 10/10\n",
            "500/500 - 10s - 20ms/step - accuracy: 0.9867 - loss: 0.0418 - val_accuracy: 0.9770 - val_loss: 0.0861\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9713 - loss: 21.7346\n",
            "Precisión prueba: 97.54% | Pérdida: 17.7625\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración utilizada:**\n",
        "- Capas ocultas: 3 capas de 256 neuronas con ReLU\n",
        "- Batch size: 100\n",
        "- Learning rate: 0.001 (Default)\n",
        "\n",
        "**Resultados:**\n",
        "- Precisión prueba: 97.54%\n",
        "- Precisión validación: 95.82%\n",
        "- Tiempo de ejecución: 18s\n",
        "\n",
        "**Observaciones:**\n",
        "\n",
        "En este caso, la implementación de dropout fue bastante exitosa. La precisión en el conjunto de prueba fue de 97.54%, sin embargo seguimos viendo una ligera tendencia al overfitting. Dónde en el conjunto de prueba observamos precisiones más altas en comparación al conjunto de validación. En búsqueda de un mejor modelo, podemos intentar implementar esta técnica en las primeras capas en conjunción con L2 e intentar que las últimas capas tengan una mayor libertad."
      ],
      "metadata": {
        "id": "OVF7PgxtEa6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DfIgFosvEZQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Visualización"
      ],
      "metadata": {
        "id": "q-NFEiTWp8gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Modelo Óptimo\n",
        "\n",
        "Con base en los resultados obtenidos en los experimentos anteriores, planteamos un modelo que cumpla con las siguientes características:\n",
        "\n",
        "- Una capa inicial ancha, seguida de capas más angostas\n",
        "- ReLU para todas las capas, potencialmente tanh para la última\n",
        "- Learning Rate cercano al default, para evitar convergencia muy temprana o tardía.\n",
        "- Implementación de dropout y L2 para evitar overfitting\n"
      ],
      "metadata": {
        "id": "CVSFNwQEp_K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Modelo con regularización moderada\n",
        "# 1. Light Regularization Model (L2 + Dropout)\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "modelo.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\")\n",
        "modelo.fit(datos_entreno,\n",
        "                   epochs=10,\n",
        "                   validation_data=datos_validacion,\n",
        "                   verbose=2)\n",
        "\n",
        "perdida, precision = modelo.evaluate(datos_prueba)\n",
        "print(f'Precisión prueba: {precision*100:.2f}% | Pérdida: {perdida:.4f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0E8HYxAy3wA",
        "outputId": "753cb03d-d48d-46f7-a2c8-fa099d0a5aa3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\n",
            "Epoch 1/10\n",
            "500/500 - 7s - 14ms/step - accuracy: 0.8965 - loss: 0.3410 - val_accuracy: 0.9644 - val_loss: 0.1178\n",
            "Epoch 2/10\n",
            "500/500 - 8s - 15ms/step - accuracy: 0.9601 - loss: 0.1384 - val_accuracy: 0.9723 - val_loss: 0.0992\n",
            "Epoch 3/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9698 - loss: 0.1001 - val_accuracy: 0.9733 - val_loss: 0.0890\n",
            "Epoch 4/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9763 - loss: 0.0782 - val_accuracy: 0.9764 - val_loss: 0.0848\n",
            "Epoch 5/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9801 - loss: 0.0649 - val_accuracy: 0.9762 - val_loss: 0.0819\n",
            "Epoch 6/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9825 - loss: 0.0565 - val_accuracy: 0.9800 - val_loss: 0.0697\n",
            "Epoch 7/10\n",
            "500/500 - 3s - 6ms/step - accuracy: 0.9839 - loss: 0.0522 - val_accuracy: 0.9756 - val_loss: 0.0855\n",
            "Epoch 8/10\n",
            "500/500 - 5s - 10ms/step - accuracy: 0.9859 - loss: 0.0438 - val_accuracy: 0.9802 - val_loss: 0.0728\n",
            "Epoch 9/10\n",
            "500/500 - 3s - 5ms/step - accuracy: 0.9875 - loss: 0.0399 - val_accuracy: 0.9796 - val_loss: 0.0780\n",
            "Epoch 10/10\n",
            "500/500 - 6s - 12ms/step - accuracy: 0.9870 - loss: 0.0412 - val_accuracy: 0.9784 - val_loss: 0.0752\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 16.9710\n",
            "Precisión prueba: 97.93% | Pérdida: 13.4832\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo inicial únicamente empleo dropout, y 3 capas con la siguiente configuración:\n",
        "\n",
        "- 512 neuronas ReLU\n",
        "- 128 neuronas ReLU\n",
        "- 128 neuronas ReLU\n",
        "\n",
        "Tuvo un rendimiento bastante bueno, sin embargo queremos seguir mejorándolo. Lo que podemos observar aquí es lo siguiente:\n",
        "\n",
        "- Ligero overfitting, dónde hay una pequeña discrepancia entre la precisión de entrenamiento y validación. Alrededor del 1%\n",
        "- El modelo parece seguir mejorando, puede que más épocas pruebe ser beneficioso\n",
        "\n",
        "Podemos plantear los siguientes cambios:\n",
        "\n",
        "- Implementar la última capa utilizando tanh\n",
        "- Aumentar la cantidad de épocas\n",
        "- Aplicar L2 a la primera capa\n",
        "- Variar el dropout para disminuir en las capas posteriores"
      ],
      "metadata": {
        "id": "_o3E_Ei6E8rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Modelo con regularización moderada\n",
        "# 1. Light Regularization Model (L2 + Dropout)\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation='tanh'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(tamanio_salida, activation='softmax')\n",
        "])\n",
        "modelo.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"Entrenamiento con regularización moderada (L2=0.001, Dropout=0.2)\")\n",
        "modelo.fit(datos_entreno,\n",
        "                   epochs=20,\n",
        "                   validation_data=datos_validacion,\n",
        "                   verbose=2)\n",
        "\n",
        "perdida, precision = modelo.evaluate(datos_prueba)\n",
        "print(f'Precisión prueba: {precision*100:.2f}% | Pérdida: {perdida:.4f}\\n')"
      ],
      "metadata": {
        "id": "5RD_DeYFGUMe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}